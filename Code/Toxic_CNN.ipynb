{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras import utils\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load cleaned datasets\n",
    "train = pd.read_csv('train_x.csv', header=None)\n",
    "train_y = pd.read_csv('train_class.csv', header=None)\n",
    "valid = pd.read_csv('test_x.csv', header=None)\n",
    "valid_y = pd.read_csv('test_class.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = train[0].tolist()\n",
    "valid_list = valid[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8103 24308\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_list),\n",
    "      len(train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabulary representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "max_seq=100\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "     \n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    tokenizer.num_words = TOP_K\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    average = sum(len(word) for word in x_train) / len(x_train)\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the end and sequences longer are truncated\n",
    "    # at the end.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length, padding= 'post')\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length, padding= 'post')\n",
    "    return x_train, x_val, tokenizer.word_index, max_length, average\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_val, word_index, max_length, average = sequence_vectorize(train_list, valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.56726180681257\n"
     ]
    }
   ],
   "source": [
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make usable label vectors\n",
    "\n",
    "y_train = np.asarray(train_y[0])\n",
    "y_valid =np.asarray(valid_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (24308, 100)\n",
      "Shape of label tensor: (24308,)\n",
      "Shape of data tensor: (8103, 100)\n",
      "Shape of label tensor: (8103,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of label tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of data tensor:', x_val.shape)\n",
    "print('Shape of label tensor:', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27290\n"
     ]
    }
   ],
   "source": [
    "vocab_size=(len(word_index))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding layers\n",
    "* GloVe\n",
    "* word2vec\n",
    "* random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#create embedding Matrix with gloVE\n",
    "\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\") #100 or 50 have to choose\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size+1, 100)) #100 = embedding_dimension or 50\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    \n",
    "      \n",
    "# non trainable        \n",
    "GoVeEmbedding = Embedding(vocab_size+1, 100, weights=[embedding_matrix], input_length=100, trainable=False )##100 or50\n",
    "\n",
    "#trainable\n",
    "GoVeEmbeddingTrain = Embedding(vocab_size+1, 100, weights=[embedding_matrix], input_length=100, trainable=True)##100 or50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create embedding Matrix with word2vec\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#load vectors\n",
    "#word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "\n",
    "#load vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit= 10 ** 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DIM=300\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "#del(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding Layer\n",
    "#non trainable\n",
    "word2vecEmbedding = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_seq,\n",
    "                            trainable=False)\n",
    "#trainable\n",
    "word2vecEmbeddingTrain = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_seq,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainable embedding layer\n",
    "\n",
    "embedding_layer_trainable= Embedding(input_dim=vocab_size,\n",
    "                                               output_dim=300,\n",
    "                                               input_length=max_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_model(filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 embedding_layer\n",
    "                ):\n",
    "\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. Defined as parameter\n",
    "\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    #dropout layer\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    #convolutional layers\n",
    "    model.add(Conv1D(filters=filters,\n",
    "                     kernel_size=kernel_size,\n",
    "                     activation='relu',\n",
    "                     input_shape= (max_seq, embedding_dim),\n",
    "                     #bias_initializer='random_uniform',\n",
    "                     #kernel_initializer='random_uniform',\n",
    "                     padding='valid'))\n",
    "    \n",
    "    model.add(Conv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  #bias_initializer='random_uniform',\n",
    "                                  #kernel_initializer='random_uniform',\n",
    "                                  padding='valid'))\n",
    "    #pooling layer\n",
    "    model.add(MaxPooling1D())\n",
    "\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Conv1D(filters=filters ,\n",
    "                              kernel_size=kernel_size+1,\n",
    "                              activation='relu',\n",
    "                              #bias_initializer='random_uniform',\n",
    "                              #kernel_initializer='random_uniform',\n",
    "                              padding='valid'))\n",
    "    model.add(Conv1D(filters=filters ,\n",
    "                              kernel_size=kernel_size+2,\n",
    "                              activation='relu',\n",
    "                              #bias_initializer='random_uniform',\n",
    "                              #kernel_initializer='random_uniform',\n",
    "                              padding='valid'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    #dense, final prediction layer, binary problem\n",
    "    model.add(Dense(1, 'sigmoid')) \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training hyperparameters\n",
    "def train_model(learning_rate,\n",
    "                epochs,\n",
    "                batch_size,\n",
    "                dropout_rate,\n",
    "                model):\n",
    "\n",
    "# Compile model with learning parameters.\n",
    "\n",
    "    loss = 'binary_crossentropy'\n",
    "\n",
    "    keras.optimizers.SGD(lr=learning_rate)\n",
    "\n",
    "    model.compile(optimizer='sgd', loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "\n",
    "#     Train and validate model.\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        #validation_data=(x_vall, y_vall),\n",
    "        validation_split=0.2,\n",
    "        verbose=1,  # Logs once per epoch.\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    return history\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "    \n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test different model variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          2729100   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 98, 120)           36120     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 96, 120)           43320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 45, 120)           57720     \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 41, 120)           72120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 2,938,501\n",
      "Trainable params: 209,401\n",
      "Non-trainable params: 2,729,100\n",
      "_________________________________________________________________\n",
      "Train on 19446 samples, validate on 4862 samples\n",
      "Epoch 1/100\n",
      "19446/19446 [==============================] - 48s 2ms/sample - loss: 0.6655 - acc: 0.5941 - val_loss: 0.6151 - val_acc: 0.6798\n",
      "Epoch 2/100\n",
      "19446/19446 [==============================] - 47s 2ms/sample - loss: 0.5315 - acc: 0.7276 - val_loss: 0.4564 - val_acc: 0.7951\n",
      "Epoch 3/100\n",
      "19446/19446 [==============================] - 48s 2ms/sample - loss: 0.4428 - acc: 0.8052 - val_loss: 0.4099 - val_acc: 0.8264\n",
      "Epoch 4/100\n",
      "19446/19446 [==============================] - 48s 2ms/sample - loss: 0.4129 - acc: 0.8196 - val_loss: 0.3830 - val_acc: 0.8344\n",
      "Epoch 5/100\n",
      "19446/19446 [==============================] - 49s 3ms/sample - loss: 0.3900 - acc: 0.8281 - val_loss: 0.3755 - val_acc: 0.8369\n",
      "Epoch 6/100\n",
      "19446/19446 [==============================] - 49s 3ms/sample - loss: 0.3722 - acc: 0.8368 - val_loss: 0.3586 - val_acc: 0.8462\n",
      "Epoch 7/100\n",
      "19446/19446 [==============================] - 48s 2ms/sample - loss: 0.3603 - acc: 0.8423 - val_loss: 0.3363 - val_acc: 0.8573\n",
      "Epoch 8/100\n",
      "19446/19446 [==============================] - 48s 2ms/sample - loss: 0.3536 - acc: 0.8443 - val_loss: 0.3362 - val_acc: 0.8548\n",
      "Epoch 9/100\n",
      "19446/19446 [==============================] - 49s 3ms/sample - loss: 0.3404 - acc: 0.8525 - val_loss: 0.3203 - val_acc: 0.8628\n",
      "Epoch 10/100\n",
      "19446/19446 [==============================] - 53s 3ms/sample - loss: 0.3328 - acc: 0.8552 - val_loss: 0.3199 - val_acc: 0.8614\n",
      "Epoch 11/100\n",
      "19446/19446 [==============================] - 49s 3ms/sample - loss: 0.3292 - acc: 0.8587 - val_loss: 0.3097 - val_acc: 0.8669\n",
      "Epoch 12/100\n",
      "19446/19446 [==============================] - 49s 3ms/sample - loss: 0.3222 - acc: 0.8586 - val_loss: 0.3085 - val_acc: 0.8686\n",
      "Epoch 13/100\n",
      "19446/19446 [==============================] - 48s 2ms/sample - loss: 0.3175 - acc: 0.8613 - val_loss: 0.3009 - val_acc: 0.8671\n",
      "Epoch 14/100\n",
      "19446/19446 [==============================] - 49s 3ms/sample - loss: 0.3130 - acc: 0.8660 - val_loss: 0.2978 - val_acc: 0.8690\n",
      "Epoch 15/100\n",
      "19446/19446 [==============================] - 49s 2ms/sample - loss: 0.3079 - acc: 0.8684 - val_loss: 0.2940 - val_acc: 0.8723\n",
      "Epoch 16/100\n",
      "19446/19446 [==============================] - 48s 2ms/sample - loss: 0.3068 - acc: 0.8656 - val_loss: 0.2932 - val_acc: 0.8717\n",
      "Epoch 17/100\n",
      "19446/19446 [==============================] - 49s 3ms/sample - loss: 0.3008 - acc: 0.8674 - val_loss: 0.2909 - val_acc: 0.8733\n",
      "Epoch 18/100\n",
      "19446/19446 [==============================] - 49s 3ms/sample - loss: 0.3003 - acc: 0.8725 - val_loss: 0.2937 - val_acc: 0.8715\n",
      "Epoch 19/100\n",
      "19446/19446 [==============================] - 52s 3ms/sample - loss: 0.2982 - acc: 0.8733 - val_loss: 0.2924 - val_acc: 0.8719\n",
      "8103/8103 [==============================] - 6s 802us/sample - loss: 0.2967 - acc: 0.8708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29670873278639104, 0.8707886]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GloVe embedding with fixed embedding\n",
    "\n",
    "model_GloVe_fix =define_model(filters=120,   \n",
    "                              kernel_size=3,\n",
    "                              embedding_dim=100,\n",
    "                              dropout_rate=0.2,\n",
    "                              pool_size=2,\n",
    "                              embedding_layer= GoVeEmbedding)\n",
    "\n",
    "model_GloVe_fix.summary()\n",
    "\n",
    "train_model(learning_rate= 0.001,\n",
    "                epochs=100,\n",
    "                batch_size=64,\n",
    "                dropout_rate=0.2,\n",
    "                model=model_GloVe_fix\n",
    "                )\n",
    "\n",
    "#evaluate model\n",
    "\n",
    "model_GloVe_fix.evaluate(x_val, y_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: \n",
      "0.8739617190321415\n",
      "Precision: \n",
      "0.8638743455497382\n",
      "Recall:\n",
      "0.8842874543239951\n",
      "Recall:\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.around(model_GloVe_fix.predict(x_val)).astype(int)\n",
    "\n",
    "print('F1 score: ')\n",
    "print(sklearn.metrics.f1_score(y_valid, y_pred))\n",
    "print('Precision: ')\n",
    "print(sklearn.metrics.precision_score(y_valid, y_pred, pos_label=1))\n",
    "print('Recall:')\n",
    "print(sklearn.metrics.recall_score(y_valid, y_pred, pos_label=1))\n",
    "\n",
    "\n",
    "# Save model.\n",
    "model_GloVe_fix.save('GloVe_fix_CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2729100   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 98, 120)           36120     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 96, 120)           43320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 45, 120)           57720     \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 41, 120)           72120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 2,938,501\n",
      "Trainable params: 2,938,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19446 samples, validate on 4862 samples\n",
      "Epoch 1/100\n",
      "19446/19446 [==============================] - 65s 3ms/sample - loss: 0.6592 - acc: 0.6072 - val_loss: 0.6022 - val_acc: 0.6783\n",
      "Epoch 2/100\n",
      "19446/19446 [==============================] - 65s 3ms/sample - loss: 0.5258 - acc: 0.7353 - val_loss: 0.4514 - val_acc: 0.7900\n",
      "Epoch 3/100\n",
      "19446/19446 [==============================] - 64s 3ms/sample - loss: 0.4420 - acc: 0.8047 - val_loss: 0.4054 - val_acc: 0.8248\n",
      "Epoch 4/100\n",
      "19446/19446 [==============================] - 64s 3ms/sample - loss: 0.4087 - acc: 0.8238 - val_loss: 0.3780 - val_acc: 0.8449\n",
      "Epoch 5/100\n",
      "19446/19446 [==============================] - 60s 3ms/sample - loss: 0.3885 - acc: 0.8304 - val_loss: 0.3630 - val_acc: 0.8525\n",
      "Epoch 6/100\n",
      "19446/19446 [==============================] - 60s 3ms/sample - loss: 0.3691 - acc: 0.8414 - val_loss: 0.3455 - val_acc: 0.8573\n",
      "Epoch 7/100\n",
      "19446/19446 [==============================] - 61s 3ms/sample - loss: 0.3582 - acc: 0.8449 - val_loss: 0.3354 - val_acc: 0.8591\n",
      "Epoch 8/100\n",
      "19446/19446 [==============================] - 61s 3ms/sample - loss: 0.3521 - acc: 0.8469 - val_loss: 0.3277 - val_acc: 0.8618\n",
      "Epoch 9/100\n",
      "19446/19446 [==============================] - 61s 3ms/sample - loss: 0.3404 - acc: 0.8522 - val_loss: 0.3280 - val_acc: 0.8628\n",
      "Epoch 10/100\n",
      "19446/19446 [==============================] - 64s 3ms/sample - loss: 0.3323 - acc: 0.8549 - val_loss: 0.3134 - val_acc: 0.8649\n",
      "Epoch 11/100\n",
      "19446/19446 [==============================] - 65s 3ms/sample - loss: 0.3252 - acc: 0.8594 - val_loss: 0.3192 - val_acc: 0.8636\n",
      "Epoch 12/100\n",
      "19446/19446 [==============================] - 66s 3ms/sample - loss: 0.3221 - acc: 0.8641 - val_loss: 0.3055 - val_acc: 0.8675\n",
      "Epoch 13/100\n",
      "19446/19446 [==============================] - 67s 3ms/sample - loss: 0.3182 - acc: 0.8626 - val_loss: 0.3003 - val_acc: 0.8712\n",
      "Epoch 14/100\n",
      "19446/19446 [==============================] - 67s 3ms/sample - loss: 0.3111 - acc: 0.8661 - val_loss: 0.3020 - val_acc: 0.8698\n",
      "Epoch 15/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.3069 - acc: 0.8704 - val_loss: 0.2977 - val_acc: 0.8725\n",
      "Epoch 16/100\n",
      "19446/19446 [==============================] - 67s 3ms/sample - loss: 0.3011 - acc: 0.8722 - val_loss: 0.3112 - val_acc: 0.8669\n",
      "Epoch 17/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.3021 - acc: 0.8717 - val_loss: 0.2943 - val_acc: 0.8749\n",
      "Epoch 18/100\n",
      "19446/19446 [==============================] - 67s 3ms/sample - loss: 0.2973 - acc: 0.8742 - val_loss: 0.2904 - val_acc: 0.8764\n",
      "Epoch 19/100\n",
      "19446/19446 [==============================] - 67s 3ms/sample - loss: 0.2956 - acc: 0.8749 - val_loss: 0.2916 - val_acc: 0.8760\n",
      "Epoch 20/100\n",
      "19446/19446 [==============================] - 71s 4ms/sample - loss: 0.2884 - acc: 0.8787 - val_loss: 0.2870 - val_acc: 0.8778\n",
      "Epoch 21/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.2836 - acc: 0.8821 - val_loss: 0.2889 - val_acc: 0.8778\n",
      "Epoch 22/100\n",
      "19446/19446 [==============================] - 68s 4ms/sample - loss: 0.2827 - acc: 0.8829 - val_loss: 0.2824 - val_acc: 0.8799\n",
      "Epoch 23/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.2845 - acc: 0.8819 - val_loss: 0.2906 - val_acc: 0.8758\n",
      "Epoch 24/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2782 - acc: 0.8808 - val_loss: 0.2838 - val_acc: 0.8789\n",
      "8103/8103 [==============================] - 7s 896us/sample - loss: 0.2898 - acc: 0.8747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28979905418234225, 0.87473774]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GloVe embedding with trainable embedding vectors\n",
    "\n",
    "model_GloVe_train =define_model(filters=120,   \n",
    "                              kernel_size=3,\n",
    "                              embedding_dim=100,\n",
    "                              dropout_rate=0.2,\n",
    "                              pool_size=2,\n",
    "                              embedding_layer= GoVeEmbeddingTrain)\n",
    "\n",
    "model_GloVe_train.summary()\n",
    "\n",
    "train_model(learning_rate= 0.001,\n",
    "                epochs=100,\n",
    "                batch_size=64,\n",
    "                dropout_rate=0.2,\n",
    "                model=model_GloVe_train\n",
    "                )\n",
    "\n",
    "#evaluate model\n",
    "\n",
    "model_GloVe_train.evaluate(x_val, y_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: \n",
      "0.87790208107783\n",
      "Precision: \n",
      "0.8671577946768061\n",
      "Recall:\n",
      "0.8889159561510354\n",
      "Recall:\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.around(model_GloVe_train.predict(x_val)).astype(int)\n",
    "\n",
    "print('F1 score: ')\n",
    "print(sklearn.metrics.f1_score(y_valid, y_pred))\n",
    "print('Precision: ')\n",
    "print(sklearn.metrics.precision_score(y_valid, y_pred, pos_label=1))\n",
    "print('Recall:')\n",
    "print(sklearn.metrics.recall_score(y_valid, y_pred, pos_label=1))\n",
    "print('Recall:')\n",
    "\n",
    "# Save model.\n",
    "model_GloVe_train.save('GloVe_train_CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 300)          8187000   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 98, 120)           108120    \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 96, 120)           43320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 45, 120)           57720     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 41, 120)           72120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 8,468,401\n",
      "Trainable params: 281,401\n",
      "Non-trainable params: 8,187,000\n",
      "_________________________________________________________________\n",
      "Train on 19446 samples, validate on 4862 samples\n",
      "Epoch 1/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.6857 - acc: 0.5522 - val_loss: 0.6799 - val_acc: 0.5794\n",
      "Epoch 2/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.6765 - acc: 0.5842 - val_loss: 0.6673 - val_acc: 0.6035\n",
      "Epoch 3/100\n",
      "19446/19446 [==============================] - 67s 3ms/sample - loss: 0.6530 - acc: 0.6230 - val_loss: 0.6155 - val_acc: 0.6715\n",
      "Epoch 4/100\n",
      "19446/19446 [==============================] - 68s 4ms/sample - loss: 0.5552 - acc: 0.7285 - val_loss: 0.4647 - val_acc: 0.8077\n",
      "Epoch 5/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.4542 - acc: 0.8003 - val_loss: 0.4144 - val_acc: 0.8244\n",
      "Epoch 6/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.4160 - acc: 0.8185 - val_loss: 0.3817 - val_acc: 0.8418\n",
      "Epoch 7/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.3891 - acc: 0.8315 - val_loss: 0.3700 - val_acc: 0.8451\n",
      "Epoch 8/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.3673 - acc: 0.8432 - val_loss: 0.3474 - val_acc: 0.8523\n",
      "Epoch 9/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.3543 - acc: 0.8500 - val_loss: 0.3354 - val_acc: 0.8573\n",
      "Epoch 10/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.3411 - acc: 0.8534 - val_loss: 0.3266 - val_acc: 0.8595\n",
      "Epoch 11/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.3284 - acc: 0.8642 - val_loss: 0.3209 - val_acc: 0.8622\n",
      "Epoch 12/100\n",
      "19446/19446 [==============================] - 71s 4ms/sample - loss: 0.3212 - acc: 0.8643 - val_loss: 0.3120 - val_acc: 0.8671\n",
      "Epoch 13/100\n",
      "19446/19446 [==============================] - 70s 4ms/sample - loss: 0.3112 - acc: 0.8695 - val_loss: 0.3072 - val_acc: 0.8671\n",
      "Epoch 14/100\n",
      "19446/19446 [==============================] - 70s 4ms/sample - loss: 0.3087 - acc: 0.8697 - val_loss: 0.3035 - val_acc: 0.8665\n",
      "Epoch 15/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2999 - acc: 0.8742 - val_loss: 0.2992 - val_acc: 0.8688\n",
      "Epoch 16/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2948 - acc: 0.8766 - val_loss: 0.2955 - val_acc: 0.8704\n",
      "Epoch 17/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2899 - acc: 0.8788 - val_loss: 0.2946 - val_acc: 0.8708\n",
      "Epoch 18/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2838 - acc: 0.8806 - val_loss: 0.2921 - val_acc: 0.8737\n",
      "Epoch 19/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2771 - acc: 0.8842 - val_loss: 0.2860 - val_acc: 0.8768\n",
      "Epoch 20/100\n",
      "19446/19446 [==============================] - 73s 4ms/sample - loss: 0.2758 - acc: 0.8852 - val_loss: 0.2839 - val_acc: 0.8770\n",
      "Epoch 21/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2705 - acc: 0.8878 - val_loss: 0.2878 - val_acc: 0.8760\n",
      "Epoch 22/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2659 - acc: 0.8890 - val_loss: 0.2830 - val_acc: 0.8772\n",
      "Epoch 23/100\n",
      "19446/19446 [==============================] - 70s 4ms/sample - loss: 0.2629 - acc: 0.8914 - val_loss: 0.2799 - val_acc: 0.8770\n",
      "Epoch 24/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2573 - acc: 0.8944 - val_loss: 0.2780 - val_acc: 0.8803\n",
      "Epoch 25/100\n",
      "19446/19446 [==============================] - 68s 4ms/sample - loss: 0.2540 - acc: 0.8943 - val_loss: 0.2782 - val_acc: 0.8776\n",
      "Epoch 26/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2507 - acc: 0.8942 - val_loss: 0.2763 - val_acc: 0.8830\n",
      "Epoch 27/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2487 - acc: 0.9000 - val_loss: 0.2725 - val_acc: 0.8826\n",
      "Epoch 28/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2466 - acc: 0.8986 - val_loss: 0.2702 - val_acc: 0.8828\n",
      "Epoch 29/100\n",
      "19446/19446 [==============================] - 68s 3ms/sample - loss: 0.2413 - acc: 0.9011 - val_loss: 0.2764 - val_acc: 0.8828\n",
      "Epoch 30/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2390 - acc: 0.9006 - val_loss: 0.2685 - val_acc: 0.8852\n",
      "Epoch 31/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2344 - acc: 0.9026 - val_loss: 0.2688 - val_acc: 0.8844\n",
      "Epoch 32/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2342 - acc: 0.9039 - val_loss: 0.2674 - val_acc: 0.8871\n",
      "Epoch 33/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2293 - acc: 0.9062 - val_loss: 0.2658 - val_acc: 0.8879\n",
      "Epoch 34/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2227 - acc: 0.9094 - val_loss: 0.2679 - val_acc: 0.8865\n",
      "Epoch 35/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2231 - acc: 0.9098 - val_loss: 0.2653 - val_acc: 0.8885\n",
      "Epoch 36/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2182 - acc: 0.9099 - val_loss: 0.2660 - val_acc: 0.8889\n",
      "Epoch 37/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2186 - acc: 0.9106 - val_loss: 0.2624 - val_acc: 0.8904\n",
      "Epoch 38/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2127 - acc: 0.9147 - val_loss: 0.2666 - val_acc: 0.8908\n",
      "Epoch 39/100\n",
      "19446/19446 [==============================] - 69s 4ms/sample - loss: 0.2096 - acc: 0.9168 - val_loss: 0.2644 - val_acc: 0.8910\n",
      "8103/8103 [==============================] - 10s 1ms/sample - loss: 0.2667 - acc: 0.8893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2666704379032236, 0.8893003]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec embedding with fixed embedding vectors\n",
    "\n",
    "model_w2v_fix =define_model(filters=120,   \n",
    "                              kernel_size=3,\n",
    "                              embedding_dim=100,\n",
    "                              dropout_rate=0.2,\n",
    "                              pool_size=2,\n",
    "                              embedding_layer= word2vecEmbedding)\n",
    "\n",
    "model_w2v_fix.summary()\n",
    "\n",
    "train_model(learning_rate= 0.001,\n",
    "                epochs=100,\n",
    "                batch_size=64,\n",
    "                dropout_rate=0.2,\n",
    "                model=model_w2v_fix\n",
    "                )\n",
    "\n",
    "#evaluate model\n",
    "\n",
    "model_w2v_fix.evaluate(x_val, y_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: \n",
      "0.888998886276451\n",
      "Precision: \n",
      "0.903420523138833\n",
      "Recall:\n",
      "0.8750304506699147\n",
      "Recall:\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.around(model_w2v_fix.predict(x_val)).astype(int)\n",
    "\n",
    "print('F1 score: ')\n",
    "print(sklearn.metrics.f1_score(y_valid, y_pred))\n",
    "print('Precision: ')\n",
    "print(sklearn.metrics.precision_score(y_valid, y_pred, pos_label=1))\n",
    "print('Recall:')\n",
    "print(sklearn.metrics.recall_score(y_valid, y_pred, pos_label=1))\n",
    "\n",
    "\n",
    "# Save model.\n",
    "model_w2v_fix.save('W2V_fix_CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 300)          8187000   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 98, 120)           108120    \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 96, 120)           43320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 45, 120)           57720     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 41, 120)           72120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 8,468,401\n",
      "Trainable params: 8,468,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19446 samples, validate on 4862 samples\n",
      "Epoch 1/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.6883 - acc: 0.5518 - val_loss: 0.6802 - val_acc: 0.5740\n",
      "Epoch 2/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.6751 - acc: 0.5864 - val_loss: 0.6668 - val_acc: 0.6006\n",
      "Epoch 3/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.6488 - acc: 0.6263 - val_loss: 0.6088 - val_acc: 0.6847\n",
      "Epoch 4/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.5460 - acc: 0.7382 - val_loss: 0.4702 - val_acc: 0.8015\n",
      "Epoch 5/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.4489 - acc: 0.7981 - val_loss: 0.4109 - val_acc: 0.8211\n",
      "Epoch 6/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.4084 - acc: 0.8206 - val_loss: 0.3804 - val_acc: 0.8404\n",
      "Epoch 7/100\n",
      "19446/19446 [==============================] - 105s 5ms/sample - loss: 0.3815 - acc: 0.8380 - val_loss: 0.3644 - val_acc: 0.8433\n",
      "Epoch 8/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.3619 - acc: 0.8449 - val_loss: 0.3466 - val_acc: 0.8519\n",
      "Epoch 9/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.3420 - acc: 0.8552 - val_loss: 0.3306 - val_acc: 0.8601\n",
      "Epoch 10/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.3326 - acc: 0.8587 - val_loss: 0.3205 - val_acc: 0.8640\n",
      "Epoch 11/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.3197 - acc: 0.8636 - val_loss: 0.3171 - val_acc: 0.8647\n",
      "Epoch 12/100\n",
      "19446/19446 [==============================] - 106s 5ms/sample - loss: 0.3116 - acc: 0.8681 - val_loss: 0.3073 - val_acc: 0.8669\n",
      "Epoch 13/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.3017 - acc: 0.8749 - val_loss: 0.3009 - val_acc: 0.8725\n",
      "Epoch 14/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.2970 - acc: 0.8754 - val_loss: 0.2964 - val_acc: 0.8712\n",
      "Epoch 15/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.2869 - acc: 0.8806 - val_loss: 0.2906 - val_acc: 0.8749\n",
      "Epoch 16/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.2809 - acc: 0.8833 - val_loss: 0.2883 - val_acc: 0.8791\n",
      "Epoch 17/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.2768 - acc: 0.8856 - val_loss: 0.2866 - val_acc: 0.8787\n",
      "Epoch 18/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.2708 - acc: 0.8901 - val_loss: 0.2793 - val_acc: 0.8795\n",
      "Epoch 19/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.2654 - acc: 0.8920 - val_loss: 0.2855 - val_acc: 0.8805\n",
      "Epoch 20/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.2582 - acc: 0.8927 - val_loss: 0.2802 - val_acc: 0.8780\n",
      "8103/8103 [==============================] - 10s 1ms/sample - loss: 0.2799 - acc: 0.8847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2799303570835146, 0.88473403]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec embedding with trainable embedding vectors\n",
    "\n",
    "model_w2v_train =define_model(filters=120,   \n",
    "                              kernel_size=3,\n",
    "                              embedding_dim=100,\n",
    "                              dropout_rate=0.2,\n",
    "                              pool_size=2,\n",
    "                              embedding_layer= word2vecEmbeddingTrain)\n",
    "\n",
    "model_w2v_train.summary()\n",
    "\n",
    "train_model(learning_rate= 0.001,\n",
    "                epochs=100,\n",
    "                batch_size=64,\n",
    "                dropout_rate=0.2,\n",
    "                model=model_w2v_train\n",
    "                )\n",
    "\n",
    "#evaluate model\n",
    "\n",
    "model_w2v_train.evaluate(x_val, y_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: \n",
      "0.8864023351982486\n",
      "Precision: \n",
      "0.8851105173670148\n",
      "Recall:\n",
      "0.8876979293544458\n",
      "Recall:\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.around(model_w2v_train.predict(x_val)).astype(int)\n",
    "\n",
    "print('F1 score: ')\n",
    "print(sklearn.metrics.f1_score(y_valid, y_pred))\n",
    "print('Precision: ')\n",
    "print(sklearn.metrics.precision_score(y_valid, y_pred, pos_label=1))\n",
    "print('Recall:')\n",
    "print(sklearn.metrics.recall_score(y_valid, y_pred, pos_label=1))\n",
    "print('Recall:')\n",
    "\n",
    "# Save model.\n",
    "#model_w2v_train.save('W2V_train_CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 300)          8187000   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 98, 120)           108120    \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 96, 120)           43320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 48, 120)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 45, 120)           57720     \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 41, 120)           72120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 8,468,401\n",
      "Trainable params: 8,468,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19446 samples, validate on 4862 samples\n",
      "Epoch 1/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6929 - acc: 0.5106 - val_loss: 0.6921 - val_acc: 0.4984\n",
      "Epoch 2/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6917 - acc: 0.5393 - val_loss: 0.6912 - val_acc: 0.5623\n",
      "Epoch 3/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6904 - acc: 0.5586 - val_loss: 0.6901 - val_acc: 0.5664\n",
      "Epoch 4/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6889 - acc: 0.5679 - val_loss: 0.6888 - val_acc: 0.5695\n",
      "Epoch 5/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6870 - acc: 0.5725 - val_loss: 0.6872 - val_acc: 0.5664\n",
      "Epoch 6/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6848 - acc: 0.5743 - val_loss: 0.6857 - val_acc: 0.5687\n",
      "Epoch 7/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6826 - acc: 0.5745 - val_loss: 0.6844 - val_acc: 0.5691\n",
      "Epoch 8/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6811 - acc: 0.5748 - val_loss: 0.6830 - val_acc: 0.5666\n",
      "Epoch 9/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6802 - acc: 0.5753 - val_loss: 0.6829 - val_acc: 0.5685\n",
      "Epoch 10/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6795 - acc: 0.5758 - val_loss: 0.6826 - val_acc: 0.5666\n",
      "Epoch 11/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6793 - acc: 0.5759 - val_loss: 0.6818 - val_acc: 0.5691\n",
      "Epoch 12/100\n",
      "19446/19446 [==============================] - 107s 5ms/sample - loss: 0.6783 - acc: 0.5767 - val_loss: 0.6825 - val_acc: 0.5691\n",
      "Epoch 13/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6780 - acc: 0.5787 - val_loss: 0.6813 - val_acc: 0.5701\n",
      "Epoch 14/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6780 - acc: 0.5787 - val_loss: 0.6803 - val_acc: 0.5728\n",
      "Epoch 15/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6771 - acc: 0.5785 - val_loss: 0.6800 - val_acc: 0.5734\n",
      "Epoch 16/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6761 - acc: 0.5797 - val_loss: 0.6788 - val_acc: 0.5755\n",
      "Epoch 17/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6752 - acc: 0.5816 - val_loss: 0.6776 - val_acc: 0.5773\n",
      "Epoch 18/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6742 - acc: 0.5816 - val_loss: 0.6761 - val_acc: 0.5804\n",
      "Epoch 19/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6721 - acc: 0.5873 - val_loss: 0.6748 - val_acc: 0.5821\n",
      "Epoch 20/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6698 - acc: 0.5888 - val_loss: 0.6717 - val_acc: 0.5932\n",
      "Epoch 21/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6670 - acc: 0.5965 - val_loss: 0.6691 - val_acc: 0.5903\n",
      "Epoch 22/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6625 - acc: 0.6043 - val_loss: 0.6612 - val_acc: 0.6074\n",
      "Epoch 23/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6541 - acc: 0.6174 - val_loss: 0.6535 - val_acc: 0.6139\n",
      "Epoch 24/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6422 - acc: 0.6396 - val_loss: 0.6338 - val_acc: 0.6584\n",
      "Epoch 25/100\n",
      "19446/19446 [==============================] - 104s 5ms/sample - loss: 0.6254 - acc: 0.6604 - val_loss: 0.6109 - val_acc: 0.6742\n",
      "Epoch 26/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.6020 - acc: 0.6833 - val_loss: 0.5843 - val_acc: 0.7036\n",
      "Epoch 27/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.5679 - acc: 0.7063 - val_loss: 0.5398 - val_acc: 0.7396\n",
      "Epoch 28/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.5260 - acc: 0.7367 - val_loss: 0.5592 - val_acc: 0.7190\n",
      "Epoch 29/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.4915 - acc: 0.7548 - val_loss: 0.4592 - val_acc: 0.7758\n",
      "Epoch 30/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.4577 - acc: 0.7748 - val_loss: 0.4506 - val_acc: 0.7877\n",
      "Epoch 31/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.4185 - acc: 0.8034 - val_loss: 0.3913 - val_acc: 0.8217\n",
      "Epoch 32/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.3892 - acc: 0.8223 - val_loss: 0.3647 - val_acc: 0.8373\n",
      "Epoch 33/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.3527 - acc: 0.8440 - val_loss: 0.3602 - val_acc: 0.8492\n",
      "Epoch 34/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.3285 - acc: 0.8632 - val_loss: 0.3542 - val_acc: 0.8540\n",
      "Epoch 35/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.3084 - acc: 0.8740 - val_loss: 0.4071 - val_acc: 0.8291\n",
      "Epoch 36/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.2878 - acc: 0.8837 - val_loss: 0.3153 - val_acc: 0.8690\n",
      "Epoch 37/100\n",
      "19446/19446 [==============================] - 102s 5ms/sample - loss: 0.2726 - acc: 0.8901 - val_loss: 0.3605 - val_acc: 0.8486\n",
      "Epoch 38/100\n",
      "19446/19446 [==============================] - 103s 5ms/sample - loss: 0.2612 - acc: 0.8950 - val_loss: 0.3241 - val_acc: 0.8680\n",
      "8103/8103 [==============================] - 10s 1ms/sample - loss: 0.3444 - acc: 0.8565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34440775403752233, 0.8564729]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random embedding with trainable embedding vectors\n",
    "\n",
    "model_random =define_model(filters=120,   \n",
    "                              kernel_size=3,\n",
    "                              embedding_dim=100,\n",
    "                              dropout_rate=0.2,\n",
    "                              pool_size=2,\n",
    "                              embedding_layer= embedding_layer_trainable)\n",
    "\n",
    "model_random.summary()\n",
    "\n",
    "train_model(learning_rate= 0.001,\n",
    "                epochs=100,\n",
    "                batch_size=64,\n",
    "                dropout_rate=0.2,\n",
    "                model=model_random\n",
    "                )\n",
    "\n",
    "#evaluate model\n",
    "\n",
    "model_random.evaluate(x_val, y_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: \n",
      "0.8444147157190636\n",
      "Precision: \n",
      "0.9364985163204748\n",
      "Recall:\n",
      "0.7688185140073082\n",
      "Recall:\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.around(model_random.predict(x_val)).astype(int)\n",
    "\n",
    "print('F1 score: ')\n",
    "print(sklearn.metrics.f1_score(y_valid, y_pred))\n",
    "print('Precision: ')\n",
    "print(sklearn.metrics.precision_score(y_valid, y_pred, pos_label=1))\n",
    "print('Recall:')\n",
    "print(sklearn.metrics.recall_score(y_valid, y_pred, pos_label=1))\n",
    "print('Recall:')\n",
    "\n",
    "# Save model.\n",
    "#model_random.save('random_CNN_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot model\n",
    "history_dict = history\n",
    "history_dict.keys()\n",
    "\n",
    "model= model_random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "#plot_model(model_random, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "#plot_model(model_random, to_file='model.png')\n",
    "tf.keras.utils.plot_model(\n",
    "    model_random, \n",
    "    to_file='model.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
